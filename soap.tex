%%%%%%%% ICML 2023 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2023} with \usepackage[nohyperref]{icml2023} above.
\usepackage{hyperref}

\newcommand{\method}{SOAP}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2023}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2023}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% justin's macros
\usepackage{mystyle}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Symbolic Optimization and Parsing}

\begin{document}

\twocolumn[
\icmltitle{Inferring and executing complex intents through dialogue}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2023
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Justin T. Chiu}{ct}
\icmlauthor{Wenting Zhao}{ct}
\icmlauthor{Derek Chen}{cu}
\icmlauthor{Alexander M. Rush}{ct}
\end{icmlauthorlist}

\icmlaffiliation{ct}{Cornell Tech}
\icmlaffiliation{cu}{Columbia University}

\icmlcorrespondingauthor{Justin T. Chiu}{chiu.justin.t@gmail.com}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Collaborative dialogue agents must balance communicative efficiency with task progress for effective collaboration.
We design a system that strategically asks questions while minimizing communication costs via symbolic planning.
Our system consists of a reader, planner, and writer:
The reader maps language to logical forms
which inform the symbolic planner's next decision,
represented as a logical form.
The writer then converts the logical form to a response,
taking into account any information not captured by the reader.
Results on DialOp, negotiation in Deal-or-no-Deal.
\end{abstract}

\section{Introduction}
Humans and robots often have complementary strengths.
For one, humans interact with the world in ways robots do not, resulting in unique problems that only humans encounter.
On the other hand, robots can easily perform complex computations that are difficult for humans.

Our goal is to design robots that collaborate with humans to solve complex problems.
Humans often know the problem specification, but may have difficulty solving the problem itself.
Describing the problem in its entirety is expensive, requiring a lot of human effort to convey.
Importantly, the marginal improvement from full information may not be worth the communicative cost.

Minimizing communication cost is of utmost importance.
Humans are prone to not engaging if communication costs are high.
Example of describing problem setups: static vs online.
In the worst case, humans would rather produce a very poor solution than spend effort fully describing a problem.

Challenges of information sharing:
How to efficiently elicit information.
Hard because it requires reasoning about what information you have, determining what information remains, and predicting its effect on the objective.

We perform model-based planning by maintaining a belief distribution over the underlying optimization problem and planning with a one-step lookahead heuristic.
Proposing a query language through which agents can request and receive information about the unknown optimization problem.
The query language is designed to be described concisely in natural language.

We propose Symbolic Optimization and Parsing (\method{}), a method that collaborates with humans by requests information incrementally, decreasing human communication costs.
\method{} parses natural language responses to logical forms, which are used to inform a symbolic optimization algorithm that plans what to say next.

We evaluate \method{} on a set of 3 dialogue tasks: the most difficult task in DialOp \citep{lin2023decision}, negotiation, and X.

\section{Related work}

One approach to online optimization is Bayesian optimization, which learns to maximize an unknown objective function,
e.g. $f(x) = \innerprod c x$, in as few evaluations of $f$ as possible.
A key assumption in Bayesian optimization is that $f$ is a black box with unknown structure.
Grey-box Bayesian optimization methods relax this assumption, improving sample complexity by taking advantage of structure in the optimization problem \citep{grey-box-bayesopt}.

Two examples of this are when the objective consists of composite functions \citep{astudillo2019bayesian} and multi-fidelity Bayesian optimization \citep{poloczek2016multiinformation,Zanjani_Foumani_2023}. 
Preferential Bayesian optimization \citep{astudillo2023qeubo}. (move to related work)

\section{Decision-oriented dialogue}
The goal of decision-oriented dialogue (DOD) is to solve an optimization problem, such as
\begin{equation}
\label{eqn:obj}
\begin{array}{ll}
\mbox{maximize} & \innerprod{w}{x}\\
\mbox{subject to} & x \in C.
\end{array}
\end{equation}
Solving this problem is straightforward when both $w$ and $C$ are known,
and the problem size (dimension $\dim(w)$ and number of constraints $|C|$) is not too large.

We consider the setting where the parameters $w$, constraints $C$, and decision variables $x$ of the optimization problem are partitioned between dialogue participants, requiring them to exchange information (about $w$ and $F$)\footnote{Preference or reward learning is the setting where $w$ is unknown. Constraint acquisition is where $C$ is unknown.} and make decisions  ($x$) collaboratively.
We take the perspective of one dialogue participant, who must communicate with other participants to solve the problem.

Describe prior and observation model.
We assume a fully factored prior over $w$, $p(w) = \prod_i p(w_i)$ with $w_i\sim N(\mu_0,1)$.
The response model, $p(y | a,w)$, models responses $y$ to actions $a$ given parameters $w$.
The action space is extended to $a \in \mcX \cup \mathcal{Q}$, the union of the decision space and query space.
Responses are yes/no or categorical.

\section{Planning}
We first focus on the setting where the parameters $w\in\R^n$ are unknown, ignoring constraints $C$.
Our goal is to choose the most informative and utility-aware action at every turn in order to select a good terminal $x\in\mcX = \set{0,1}^n$.

We plan by optimizing the knowledge gradient acquisition function \citep{kg}.
The knowledge gradient is an acquisition function with one-step lookahead.
It chooses the next action that gathers information which results in the largest improvement of a subsequent decision.
The subsequent decision is not restricted to previously observed values,
which is especially important in our setting where actions may not be in the decision space $\mcX$.
Formally, the knowledge gradient acquisition function is given by the following:
\begin{equation}
\label{eqn:kg}
\begin{aligned}
\argmax_a \; & \Es{y|h,a}{\max_x \mu_{w|h,a,y}(x)}\\
&- \max_{x} \mu_{w|h}(x),
\end{aligned}
\end{equation}
where $\mu_w(x) = \Es{w}{\langle w,x \rangle}$, the mean reward.
Costs: subtract (textbook) or divide (cost-aware multi-fidelity BO)?

The acquisition function in equation \ref{eqn:kg} has an argmax over actions $a \in \mcX \cup \mathcal{Q}$.
In the worst case this means that we must solve the inner problem
$\max_x \mu_{w|a,y}(x)$ for each action $a$ and response $y$,
which requires calling a solver each time.
How to prevent computational blowup?
Common to sample $y$, but that isn't enough.
$x$ is also discrete. Is this easy in weighted linear sum assignment problems?

\section{Query language}
This section discusses the query language $\mathcal{Q}$ and observation model.
Bradley-Terry and Plackett-Luce for pairwise and ranking comparisons respectively.

\begin{equation}
p(x \succ y | w)= \frac{e^{w_x}}{e^{w_x} + e^{w_y}}
\end{equation}

\section{Questions}
\begin{enumerate}
    \item Is there a way to not call the solver $|a|*|y|$ times? Some problem-dependent linear optimization trick?
    \item Is there a principled way of assigning cost to actions? Learned from static data, since we can estimate the utility of any given action? Can be a very primitive kind of learning, e.g. linear function of size($a$).
\end{enumerate}

% good baseline: normal BO w/ $a \in \mcX$
% not much cost learnign in BO. cost-aware multi-fidelity. cost model is simple from empirical study. typically rough empirical cost model would work
% we have offline data! 
% can we estimate how important cost is?

are there other acquisition functions that are 1-step lookahead but aren't intractable like KG?
can still do EI w/ 1-step lookahead. (pointers to lit would be helpful!)
lit: http://proceedings.mlr.press/v124/lee20a/lee20a.pdf


\bibliography{soap}
\bibliographystyle{icml2023}


% Acknowledgements should only appear in the accepted version.
\section*{Acknowledgements}
David Bick, Saujas Vaduguru, Xinran Zhu


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{EHIG}
Recent work has presented a unified perspective on BOpt to decision-theoretic entropies, proposing a framework that allows for the principled derivation of acquisition functions for custom tasks \citep{neiswanger2022generalizing}.
This is referred to as the expected $H_{l,A}$-information gain (EHIG),
where $H_{l,A}$ is a utility $l$ and action $A$ aware generalization of the Shannon entropy.
While this allows for unifying entropy search and BOpt, not sure if we can do anything with it yet.


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
